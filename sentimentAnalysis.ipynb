{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darrell/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One day, my younger son came to me and demanded his share of the inheritance', 'It broke my heart, but I gave him what he asked for, knowing I couldn’t stop him from making his own choices', 'He left, and I waited, hoping that one day he would return', 'I heard stories about his reckless living and the hardships he faced', \"My heart ached for him, but I couldn't force him back\", 'Then one day, while I was looking out toward the horizon, I saw him coming home', 'My heart leapt with compassion', 'I ran to him without hesitation', \"He began to apologize, but I didn't care about his past mistakes\", 'I was just overjoyed to have him back', 'I told my servants to prepare a feast, clothe him in the best robe, and put a ring on his finger', 'My son, who I thought was lost forever, had returned', 'He was dead to the world, but now he was alive again', 'We had to celebrate', 'Yet, my other son felt hurt and angry', 'I reminded him that everything I have is already his, but the return of his brother was a special moment—he was lost and now found', 'My love for both my sons remains strong, but this celebration was necessary for the joy of reunion and forgiveness']\n",
      "Working on sentence: One day, my younger son came to me and demanded his share of the inheritance\n",
      "Logits shape: torch.Size([1, 28])\n",
      "Length of emotions: 27\n",
      "Length of probabilities: 27\n",
      "Working on sentence: It broke my heart, but I gave him what he asked for, knowing I couldn’t stop him from making his own choices\n",
      "Logits shape: torch.Size([1, 28])\n",
      "Length of emotions: 27\n",
      "Length of probabilities: 27\n",
      "Working on sentence: He left, and I waited, hoping that one day he would return\n",
      "Logits shape: torch.Size([1, 28])\n",
      "Length of emotions: 27\n",
      "Length of probabilities: 27\n",
      "Working on sentence: I heard stories about his reckless living and the hardships he faced\n",
      "Logits shape: torch.Size([1, 28])\n",
      "Length of emotions: 27\n",
      "Length of probabilities: 27\n",
      "Working on sentence: My heart ached for him, but I couldn't force him back\n",
      "Logits shape: torch.Size([1, 28])\n",
      "Length of emotions: 27\n",
      "Length of probabilities: 27\n",
      "Working on sentence: Then one day, while I was looking out toward the horizon, I saw him coming home\n",
      "Logits shape: torch.Size([1, 28])\n",
      "Length of emotions: 27\n",
      "Length of probabilities: 27\n",
      "Working on sentence: My heart leapt with compassion\n",
      "Logits shape: torch.Size([1, 28])\n",
      "Length of emotions: 27\n",
      "Length of probabilities: 27\n",
      "Working on sentence: I ran to him without hesitation\n",
      "Logits shape: torch.Size([1, 28])\n",
      "Length of emotions: 27\n",
      "Length of probabilities: 27\n",
      "Working on sentence: He began to apologize, but I didn't care about his past mistakes\n",
      "Logits shape: torch.Size([1, 28])\n",
      "Length of emotions: 27\n",
      "Length of probabilities: 27\n",
      "Working on sentence: I was just overjoyed to have him back\n",
      "Logits shape: torch.Size([1, 28])\n",
      "Length of emotions: 27\n",
      "Length of probabilities: 27\n",
      "Working on sentence: I told my servants to prepare a feast, clothe him in the best robe, and put a ring on his finger\n",
      "Logits shape: torch.Size([1, 28])\n",
      "Length of emotions: 27\n",
      "Length of probabilities: 27\n",
      "Working on sentence: My son, who I thought was lost forever, had returned\n",
      "Logits shape: torch.Size([1, 28])\n",
      "Length of emotions: 27\n",
      "Length of probabilities: 27\n",
      "Working on sentence: He was dead to the world, but now he was alive again\n",
      "Logits shape: torch.Size([1, 28])\n",
      "Length of emotions: 27\n",
      "Length of probabilities: 27\n",
      "Working on sentence: We had to celebrate\n",
      "Logits shape: torch.Size([1, 28])\n",
      "Length of emotions: 27\n",
      "Length of probabilities: 27\n",
      "Working on sentence: Yet, my other son felt hurt and angry\n",
      "Logits shape: torch.Size([1, 28])\n",
      "Length of emotions: 27\n",
      "Length of probabilities: 27\n",
      "Working on sentence: I reminded him that everything I have is already his, but the return of his brother was a special moment—he was lost and now found\n",
      "Logits shape: torch.Size([1, 28])\n",
      "Length of emotions: 27\n",
      "Length of probabilities: 27\n",
      "Working on sentence: My love for both my sons remains strong, but this celebration was necessary for the joy of reunion and forgiveness\n",
      "Logits shape: torch.Size([1, 28])\n",
      "Length of emotions: 27\n",
      "Length of probabilities: 27\n",
      "admiration 0.3946595476830707\n",
      "amusement 0.2812204996452612\n",
      "anger 0.36119456216692924\n",
      "annoyance 0.33860179869567647\n",
      "approval 0.47807005310759826\n",
      "caring 0.5836239088984096\n",
      "confusion 0.48767882804660234\n",
      "curiosity 0.5439690474201652\n",
      "desire 0.6030360267442816\n",
      "disappointment 0.3915467959116487\n",
      "disapproval 0.38343343445483374\n",
      "disgust 0.26829557361848216\n",
      "embarrassment 0.37413717324242873\n",
      "excitement 0.5433427060351652\n",
      "fear 0.30142177641391754\n",
      "gratitude 0.39274683331742005\n",
      "grief 0.49690589089603987\n",
      "joy 0.30462830535629215\n",
      "love 0.320462200571509\n",
      "nervousness 0.3707500424455194\n",
      "optimism 0.3480449239997303\n",
      "pride 0.3934212458484313\n",
      "realization 0.6453307200880611\n",
      "relief 0.3473771613310365\n",
      "remorse 0.5916213016299641\n",
      "sadness 0.5168001463308054\n",
      "surprise 0.5917130743756014\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"joeddav/distilbert-base-uncased-go-emotions-student\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"joeddav/distilbert-base-uncased-go-emotions-student\")\n",
    "\n",
    "# Define the emotions (GoEmotions has 27 labels, ignoring the extra class)\n",
    "emotions = [\n",
    "    \"admiration\", \"amusement\", \"anger\", \"annoyance\", \"approval\", \"caring\", \"confusion\", \"curiosity\",\n",
    "    \"desire\", \"disappointment\", \"disapproval\", \"disgust\", \"embarrassment\", \"excitement\", \"fear\",\n",
    "    \"gratitude\", \"grief\", \"joy\", \"love\", \"nervousness\", \"optimism\", \"pride\", \"realization\",\n",
    "    \"relief\", \"remorse\", \"sadness\", \"surprise\"\n",
    "]\n",
    "\n",
    "sons_pov = \"\"\"I asked my father for my share of the inheritance and left to live freely, but I wasted everything. When a famine hit, I was left hungry and desperate. I decided to return home, ashamed, and beg my father to take me back as a servant. To my surprise, my father welcomed me with love, embraced me, and celebrated my return, even though I didn't deserve it.\"\"\"\n",
    "sons_pov_longer = \"\"\"I was tired of living under my father's roof, so I demanded my share of the inheritance. I wanted freedom and control over my own life. My father granted my request and gave me my share of the estate. I took everything and went to a distant land, where I lived extravagantly, spending recklessly on pleasures. It wasn't long before I had nothing left. Then, a severe famine struck, and I found myself penniless and hungry. Out of desperation, I found a job feeding pigs, but I was so hungry that I longed to eat the pig’s food. That’s when it hit me—my father's servants had more than enough to eat, yet here I was starving. I decided to go back to my father, prepared to beg him to take me in, not as his son, but as a servant. I rehearsed my apology the entire way. But as I approached, I saw my father running towards me. Before I could even finish my confession, he embraced me and kissed me. I was overwhelmed when he called for a celebration, dressing me in fine clothes and preparing a feast. I had expected judgment but received forgiveness, love, and joy instead. I felt undeserving, but my father welcomed me back with open arms.\"\"\"\n",
    "\n",
    "fathers_pov = \"\"\"My younger son asked for his inheritance, left, and lost everything. Yet, when he came back, I saw him from afar and felt overwhelming compassion. I ran to him, embraced him, and threw a celebration because he was lost and now has returned, alive. My older son was upset, but I had to explain that the return of his brother, who was once lost, is a moment of joy for all of us.\"\"\"\n",
    "fathers_pov_longer = \"\"\"One day, my younger son came to me and demanded his share of the inheritance. It broke my heart, but I gave him what he asked for, knowing I couldn’t stop him from making his own choices. He left, and I waited, hoping that one day he would return. I heard stories about his reckless living and the hardships he faced. My heart ached for him, but I couldn't force him back. Then one day, while I was looking out toward the horizon, I saw him coming home. My heart leapt with compassion. I ran to him without hesitation. He began to apologize, but I didn't care about his past mistakes. I was just overjoyed to have him back. I told my servants to prepare a feast, clothe him in the best robe, and put a ring on his finger. My son, who I thought was lost forever, had returned. He was dead to the world, but now he was alive again. We had to celebrate. Yet, my other son felt hurt and angry. I reminded him that everything I have is already his, but the return of his brother was a special moment—he was lost and now found. My love for both my sons remains strong, but this celebration was necessary for the joy of reunion and forgiveness.\"\"\"\n",
    "\n",
    "brothers_pov = \"\"\"I’ve always obeyed my father and worked hard, yet he never threw a celebration for me. When my younger brother wasted everything and came back, my father immediately welcomed him and threw a feast. I was angry and felt it was unfair. My father reminded me that I’ve always had his love, but we should celebrate because my brother, who was lost, has come back home.\"\"\"\n",
    "\n",
    "text = fathers_pov_longer\n",
    "\n",
    "sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "print(sentences)\n",
    "\n",
    "emotion_prop_avg_acc = [0] * 27\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(f'Working on sentence: {sentence}')\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    # Perform emotion detection\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Get the logits\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Print the shape of the logits\n",
    "    print(f\"Logits shape: {logits.shape}\")\n",
    "\n",
    "    # Apply sigmoid to get probabilities\n",
    "    probabilities = torch.sigmoid(logits).detach().numpy()[0]\n",
    "\n",
    "    # Ignore the extra logit\n",
    "    probabilities = probabilities[:27]  # Slice to match the 27 emotions\n",
    "\n",
    "    for i in range(len(probabilities)):\n",
    "        emotion_prop_avg_acc[i] += probabilities[i]\n",
    "\n",
    "    # Debugging: Print lengths to ensure consistency\n",
    "    print(f\"Length of emotions: {len(emotions)}\")\n",
    "    print(f\"Length of probabilities: {len(probabilities)}\")\n",
    "\n",
    "    # Get the predicted emotions with probabilities for all emotions\n",
    "    predicted_emotions = [(emotions[i], probabilities[i]) for i in range(len(probabilities))]\n",
    "\n",
    "    # Print the results\n",
    "    # print(f\"Text: {sentence}\")\n",
    "    # print(\"Predicted Emotions and Probabilities:\")\n",
    "    # for emotion, prob in predicted_emotions:\n",
    "    #     print(f\"  {emotion}: {prob:.4f}\")\n",
    "\n",
    "for i in range(len(emotion_prop_avg_acc)):\n",
    "    emotion_prop_avg_acc[i] = emotion_prop_avg_acc[i] / len(sentences)\n",
    "    print(f\"{emotions[i]} {emotion_prop_avg_acc[i]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
